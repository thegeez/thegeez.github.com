<h1 id="post-title">Bayesian Inference with Markov Chain Monte Carlo
  in Clojure</h1>

<p>I've been working through the
  book <a href="https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers">Probabilistic
  Programming and Bayesian Methods for Hackers</a> (ProbHack)
  by Cam Davidson-Pilon. This online book is an introduction to
  bayesian methods and probabilistic programming for
  programmers. Its main focus is Bayesian Inference with Markov Chain
  Monte Carlo (BI-MCMC). The ProbHack book uses Python and the PyMC library. For my own
  understanding of the inner-workings of BI-MCMC I ported the code to
  Clojure.</p>

<p>Rather than requiring intricate mathematical knowledge to be able to
derive a numerical solution, BI-MCMC uses an algorithm to iteratively
arrive at the solution. It even turns out that the iterative solution
is needed for cases where deriving the solution is impossible. I don't
  know whether the fourth order partial derivative of co-prime free
  variables are convex and therefore have a closed form solution for
  the global optima. I do know how to code a loop. BI-MCMC luckily requires more knowledge of the latter rather than the former.</p>

<p>Bayesian Inference refers to a Bayesian model where probability
distributions produce data. The inference part is to find the
parameters of the probability distributions. Usually the answers for
the parameters are probability distributions themselves. The answers
will not be single numbers, but rather distributions that show
certainty (or uncertainty) and deviations for the answers.</p>

<p>The Markov Chain Monte Carlo (MCMC) part is the iterative algorithm that can
find the probability distributions for the parameters of the Bayesian
model using simulation and sampling. A truly naive iterative
algorithm would try every possible combination of the possible values
for the parameters. Through combinatorial explosion this is unfeasible
for most interesting models.
MCMC only considers part of the solution space.</p>


[Sidebar]
Similarity to Newton's method from SICP
Start somewhere
Check how good it is
Change position
Try again
Check convergence
Example here: http://mitpress.mit.edu/sicp/full-text/sicp/book/node12.html
[/Sidebar]

<p>The main example in the ProbHack book is an "sms/texts received per
day" received example:
<img src="/images/probhack_text_orig.png" width="570"/>
Image from: <a href="https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers">ProbHack</a>
</p>

<p>The idea is that the number of texts per day is defined by a
  Bayesian model. The question is: what are the parameters of that
  Bayesian model? BI-MCMC can answer that question by giving a
  probability distribution for the parameters. Note that the Bayesian
  model will be defined in terms of probability distributions and that
  the parameters of those distributions are probability distributions themselves.</p>

<p>The model for the texts example looks like this:
<img src="/images/sms_model.png" width="570"/>
For all days before tau the expected number of text messages is a
Poisson distribution with parameter lambda1. For all days after tau
the expected number of text messages is a Poisson distribution with
parameter lambda2. The question we want answered is when day tau is
and what the likely values for lambda1 and 2 are. In the ProbHack book
the real world reason for the detected change in number of texts per
day is given.</p>

<p>The way BI-MCMC works is basically trying a lot of different
  combinations for the values of tau, lambda1 and lambda2. Each of
  those combinations is a sample. At the end of the simulation all the
  values for the parameters are the probability distribution for the
  parameters. These can be visualized with histograms for
  example. This works because the samples are generated in such a way
  that there are more samples created with more likely values for each
  of the parameters.</p>
<p>The Markov Chain part of BI-MCMC refers to the
  fact that a new sample is generated based only on the previous
  sample. For example consider this sample (sample 60000 out of 100000):</p>

Why this works is explained in detail in the ProbHack book. 
Again, the ProbHack book is an amazing resource.
In order to fully grasp the algorithm I decided to port it to Clojure.
The software used in ProbHack is PyMC. Some of the parts have been
copied.

Get the answers to the what a parameter should be by constructing a
histograms for its distribution. A normalized histogram approaches the
actual distribution! (coin flip example or two dice example or something?)


Samples, generating a Histogram from this read the solution.
Bayesian inference is simply updating your beliefs after considering new evidence.
Because of samples the step function is the core of the code.
This is done in a functional way, rather through assignment in PyMC.
Bayes is updating beliefs considering new evidence. In bi-mcmc the
samples update our belief about the distributions for the parameters of
the model

Example from ProbHack book:

Three steps of the MCMC process
accepted, accepted-by-flip, rejected
Use text data as example, has a shorter running time than the cluster
examples in the clj code
sample, step size, accept reject, logp
Use graphs of partial histograms

I found it very difficult to understand how the observed data is
used. This is why I ported the code to Clojure to make sure my
understanding would work.

When you have a bayes model you can generate data. When you have a
model and the data, can you get the parameters? Probabilistic
programming can go forwards and backwards. 

The problem is: I have observed data that is generated by this
probabilistic model. And I want to know the parameters of this model,
or rather the parameters of the distributions of the model. The answer
will be probability distributions themselves.

The proposed distributions for the parameters in the model do not need
to be the same shape as the final answer. For instance replace lambda1
with a normal-distribution. The actual value does need to be in
range. Rather than a loop form 0 to 100 you spend more time in regions
you think are more plausible. This h

Code structure And python vs clojure discussion:
Defining a model
The python code hijacks assignment and claims "it reads just like
python code" I am not a fan of that.
The specification in Clojure does not read like anything else in
CLojure as it does not promote modeling of mutable object graphs. In
the implementation as well the structure and the values over time are separated.

Code to do:
Faster, better representation of models
Multi threading!
Different markov chains can be mixed in the samples. With the split of
the structure and the values at each step the clojure code is more
amendable to be multithreaded than the multable object graph approach
of the PyMCMC code. Of course the PyMCMC is production ready, whilst
the Clojure code is a hacked proof of concept.

Incanter prob distributions
Cern is anti-functional, the wrapping is awkward. 
MCMC needs probability distributions where the parameters change a
lot.
To get a likelyhood the code actually calls .setState on a
distribution, whereas a static method that uses the parameters and
value as arguments would be more appropriate. 
Something like [whatever the static function lib is] would be more appropriate.

<p>BI-MCMC is a way to find the parameters of a statistical model
  through Monte Carlo simulation rather than mathematical
  analysis. The algorithm mimics a common practical programmers
  approach of "lets try and see if it works" and the theory proves that this approach is
  correct. The problem requires generating and analyzing data for
  which Clojure and its features are a good fit, together with the
  help of probabilistic programming from the Incanter library.</p>


Meta-learning of step rate
As is common with all machine learning things, every parameter needs
another parameters, usually this parameter is just guessed or given
without motivation or simply learned as well. 

Conclusion
ProbHack references both AntiFragile and SignalNoise, that is good
enough for me.

Resources:
ProbHack
MatCelcey
PyMCMC docs

 

<p>I did most of the problems in emacs by copy-pasting the
  test cases from each problem page. To turn the text on
  4clojure into a runnable test I used this
  utility: <a href="https://gist.github.com/thegeez/8352754">gist:
  4clojure in editor</a>.</p>
