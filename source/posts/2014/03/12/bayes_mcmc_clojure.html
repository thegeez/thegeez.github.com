<h1 id="post-title">Bayesian Inference with Markov Chain Monte Carlo
  in Clojure</h1>

Bayesian Inference with Markov Chain Monte Carlo is a way to find the parameters of a statistical model through Monte Carlo simulation rather than mathematical analysis. 
The algorithm mimics a common practical programmers approach of "lets try and see if it works" and the theory proves that this approach is correct. The problem requires generating and analyzing data for which Clojure and its features are a good fit, together with the help of probabilistic programming from the Incanter library.
The ProbHack book is awesome. Probabilistic Programming</p>

Rather than requiring intricate mathematical knowledge to be able to
derive a numerical solution, it uses an algorithm to iteratively
arrive at the solution. It even turns out that the iterative solution
is needed for cases where deriving the solution is impossible.
As it says on ProbHack: "An introduction to Bayesian methods +
probabilistic programming with a computation/understanding-first,
mathematics-second point of view"


Bayesian Inference refers to a Bayesian model where probability
distributions produce data. The inference part is to find the
parameters of the probability distributions. Usually the answers for
the parameters are probability distributions themselves. The answers
will not be single numbers, but rather distributions that show
certainty (or uncertainty) and deviations for the answers.

The Markov Chain Monte Carlo part is the iterative algorithm that can
find the probability distributions for the parameters of the Bayesian
model using simulation and sampling. A truly naive iterative
algorithm would try every possible combination of the possible values
for the parameters. Through combinatorial explosion this is unfeasible
for most interesting models.
MCMC only considers part of the solution space.


[Sidebar]
Similarity to Newton's method from SICP
Start somewhere
Check how good it is
Change position
Try again
Check convergence
Example here: http://mitpress.mit.edu/sicp/full-text/sicp/book/node12.html
[/Sidebar]

Again, the ProbHack book is an amazing resource.
In order to fully grasp the algorithm I decided to port it to Clojure.
The software used in ProbHack is PyMC. Some of the parts have been
copied.

Samples, generating a Histogram from this read the solution.
Bayesian inference is simply updating your beliefs after considering new evidence.
Because of samples the step function is the core of the code.
This is done in a functional way, rather through assignment in PyMC.
Bayes is updating beliefs considering new evidence. In bi-mcmc the
samples update our belief about the distributions for the parameters of
the model

Three steps of the MCMC process
Use text data as example, has a shorter running time than the cluster
examples in the clj code
sample, step size, accept reject, logp
Use graphs of partial histograms

I found it very difficult to understand how the observed data is
used. This is why I ported the code to Clojure to make sure my
understanding would work.

When you have a bayes model you can generate data. When you have a
model and the data, can you get the parameters? Probabilistic
programming can go forwards and backwards. 

The problem is: I have observed data that is generated by this
probabilistic model. And I want to know the parameters of this model,
or rather the parameters of the distributions of the model. The answer
will be probability distributions themselves.

Code structure And python vs clojure discussion:
Defining a model
The python code hijacks assignment and claims "it reads just like
python code" I am not a fan of that.
The specification in Clojure does not read like anything else in
CLojure as it does not promote modeling of mutable object graphs. In
the implementation as well the structure and the values over time are separated.

Code to do:
Faster, better representation of models
Multi threading!
Different markov chains can be mixed in the samples. With the split of
the structure and the values at each step the clojure code is more
amendable to be multithreaded than the multable object graph approach
of the PyMCMC code. Of course the PyMCMC is production ready, whilst
the Clojure code is a hacked proof of concept.

Incanter prob distributions
Cern is anti-functional, the wrapping is awkward. 
MCMC needs probability distributions where the parameters change a
lot.
To get a likelyhood the code actually calls .setState on a
distribution, whereas a static method that uses the parameters and
value as arguments would be more appropriate. 
Something like [whatever the static function lib is] would be more appropriate.


Meta-learning of step rate
As is common with all machine learning things, every parameter needs
another parameters, usually this parameter is just guessed or given
without motivation or simply learned as well. 

Conclusion
ProbHack references both AntiFragile and SignalNoise, that is good
enough for me.

Resources:
ProbHack
MatCelcey
PyMCMC docs

 

<p>I did most of the problems in emacs by copy-pasting the
  test cases from each problem page. To turn the text on
  4clojure into a runnable test I used this
  utility: <a href="https://gist.github.com/thegeez/8352754">gist:
  4clojure in editor</a>.</p>
